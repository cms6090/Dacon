0. ë¨¼ì €, í‰ê°€ì§€í‘œë¥¼ ëª¨ë¸ ê´€ì ì—ì„œ ë‹¤ì‹œ ì •ì˜
ëŒ€íšŒ ì„¤ëª… + ìµœê·¼ gLM ë…¼ë¬¸ë“¤ì„ ë³´ë©´, ì´ ëŒ€íšŒ í‰ê°€ì§€í‘œëŠ” ì‚¬ì‹¤ìƒ ì´ëŸ° ê±¸ ìš”êµ¬í•˜ëŠ” ì…ˆì´ì•¼:


CD:
ê°™ì€ ìœ ì „ì ì˜ì—­ì—ì„œ ref vs variant ì„ë² ë”© ì½”ì‚¬ì¸ ê±°ë¦¬ í‰ê· 
â†’ â€œì‘ì€ SNV/indelì„ ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ë¯¼ê°í•˜ê²Œ êµ¬ë¶„í•˜ëŠ”ì§€â€ë¥¼ ë³´ëŠ” ê¸°ë³¸ variant-sensitivity


CDD:
ë³‘ì (Pathogenic) vs ì–‘ì„±(Benign) ë³€ì´ì— ëŒ€í•´

í‰ê·  ì½”ì‚¬ì¸ ê±°ë¦¬ ì°¨ì´
â†’ ë³‘ì  ë³€ì´ëŠ” refì™€ í›¨ì”¬ ë©€ê²Œ, ì–‘ì„± ë³€ì´ëŠ” refì™€ ê°€ê¹ê²Œ ì„ë² ë”© ë˜ë©´ ì¢‹ìŒ
ë¹„ìŠ·í•œ ì•„ì´ë””ì–´ê°€ Nucleotide Transformer, HyenaDNA, GPN ê³„ì—´ì—ì„œ zero-shot variant effect í‰ê°€ë¡œ ì“°ì´ê³  ìˆìŒ.



PCC:
ê° reference ì„œì—´ì— ëŒ€í•´ â€œë³€ì´ ê°œìˆ˜â€ê°€ ì¦ê°€í• ìˆ˜ë¡ ì„ë² ë”© ê±°ë¦¬ë„ ì»¤ì ¸ì•¼ í•¨
â†’ í•œ ì—¼ê¸°, ë‘ ì—¼ê¸°, ì„¸ ì—¼ê¸°â€¦ ë°”ê¿”ê°€ë©´ì„œ ê±°ë¦¬ì˜ ë‹¨ì¡° ì¦ê°€ / ê±°ì˜ ì„ í˜• ê´€ê³„ê°€ ìœ ì§€ë˜ë©´ ìœ ë¦¬


ì¦‰, ìš°ë¦¬ê°€ ì„¤ê³„í•´ì•¼ í•˜ëŠ” gLMì€:


SNV ìˆ˜ì¤€ì˜ ì‘ì€ ë³€í™”ë„ ì„ë² ë”© ê°ë„(ì½”ì‚¬ì¸ ê±°ë¦¬)ì— ì˜ ë°˜ì˜ë˜ê²Œ ë§Œë“¤ê³ 


ë³‘ì  vs ì–‘ì„± ë³€ì´ì— ëŒ€í•´ ê±°ë¦¬ ë¶„í¬ë¥¼ ìµœëŒ€í•œ ë¶„ë¦¬ì‹œí‚¤ê³ 


ë³€ì´ ê°œìˆ˜(â€œmutational burdenâ€)ì™€ ì„ë² ë”© ê±°ë¦¬ ì‚¬ì´ì˜ ìƒê´€ì„ í¬ê²Œ ë§Œë“œëŠ”


ë°©í–¥ìœ¼ë¡œ í•™ìŠµë˜ì–´ì•¼ í•´.

1. ë°ì´í„° ì „ëµ: ì™¸ë¶€ ë°ì´í„°ë¡œ â€œí‰ê°€ì§€í‘œë¥¼ í‰ë‚´ë‚´ëŠ”â€ í•™ìŠµ ì„¸íŠ¸ ë§Œë“¤ê¸°
ëŒ€íšŒì—ì„œ trainì´ ì—†ìœ¼ë‹ˆê¹Œ, í‰ê°€ì§€í‘œ êµ¬ì¡°ì™€ ìµœëŒ€í•œ ë¹„ìŠ·í•œ ì™¸ë¶€ ë°ì´í„° êµ¬ì„±ì„ ë¨¼ì € ì¡ëŠ” ê²Œ ì¤‘ìš”í•´.
1-1. ì¶”ì²œ ì™¸ë¶€ ë°ì´í„° ì†ŒìŠ¤ (ê·œì¹™ ì¶©ì¡± ì „ì œ)
ëª¨ë‘ 2025ë…„ 11ì›” 9ì¼ ì´ì „ì— ê³µê°œë˜ê³ , ë¹„ìƒì—…/ìƒì—… í—ˆìš© ë¼ì´ì„ ìŠ¤ê°€ ëª…í™•í•œ ê²ƒë§Œ ì¨ì•¼ í•¨. ì•„ë˜ëŠ” â€œë³€ì´ â†” ref ì„œì—´ + ë³‘ì /ì–‘ì„± ë¼ë²¨â€ì„ ë™ì‹œì— ì œê³µí•˜ëŠ” ëŒ€í‘œì ì¸ ì†ŒìŠ¤ë“¤:


ClinVar: ì¸ê°„ ë³€ì´ì˜ pathogenic / benign / VUS ë¼ë²¨


ì—¬ëŸ¬ gLM / PLM ê¸°ë°˜ pathogenicity ë…¼ë¬¸ì—ì„œ í‘œì¤€ì²˜ëŸ¼ ì‚¬ìš© ì¤‘.




BEND / GenomicBenchmarksì˜ Variant Effect Tasks


BENDëŠ” ì—¬ëŸ¬ noncoding variant effect taskì—ì„œ ref/alt ì„œì—´ ìŒ + ì‹¤í—˜ì  íš¨ê³¼ê°’ì„ ì œê³µí•˜ê³ , ì¼ë¶€ íƒœìŠ¤í¬ì—ì„œëŠ” ì½”ì‚¬ì¸ ê±°ë¦¬ ìì²´ë¥¼ predictorë¡œ ì”€.




Nucleotide Transformer / HyenaDNAì—ì„œ ì‚¬ìš©í•œ VEP(Variant Effect Prediction) ë°ì´í„°ì…‹


ë…¼ë¬¸ ë° GitHubÂ·HF repoì— íƒœìŠ¤í¬ êµ¬ì„±ê³¼ ë°ì´í„° ê²½ë¡œê°€ ê³µê°œë¼ ìˆìŒ. ì—¬ê¸°ì„œë„ â€œref vs mutant embedding ê±°ë¦¬â€ê°€ VEP ì ìˆ˜ë¡œ ì“°ì„.





ì‚¬ìš© ì „ì—ëŠ”: **ë°ì´í„° ê³µê°œ ì‹œì  + ë¼ì´ì„ ìŠ¤(íŠ¹íˆ ìƒì—…/ë¹„ìƒì—…)**ë¥¼ í•œ ë²ˆ ë” í™•ì¸í•˜ëŠ” ê²Œ ì¢‹ìŒ.

1-2. ìš°ë¦¬ê°€ ë§Œë“¤ â€œí•™ìŠµìš© ë°ì´í„° êµ¬ì¡°â€
ì´ ëŒ€íšŒë¥¼ ê²¨ëƒ¥í•´ì„œ, ìµœì†Œ ì•„ë˜ ìˆ˜ì¤€ì˜ í…Œì´ë¸” êµ¬ì¡°ë¥¼ ì¶”ì²œí• ê²Œ:
â‘  variant_table (ë³€ì´ ë©”íƒ€ ì •ë³´)
ì»¬ëŸ¼ëª…ì„¤ëª…var_idë³€ì´ ID (ê³ ìœ  í‚¤)ref_idí•´ë‹¹ ë³€ì´ê°€ ì†í•œ reference ì„œì—´ IDchrom, posì—¼ìƒ‰ì²´ / ìœ„ì¹˜ (ìˆìœ¼ë©´ ì¢‹ìŒ)ref_allele, alt_alleleì—¼ê¸° ìˆ˜ì¤€ ë³€ì´ ì •ë³´consequencemissense / synonymous / splice / promoter ë“±patho_label{0: benign, 1: pathogenic}n_mut_sitesì´ variantê°€ refì™€ ë‹¤ë¥¸ ì—¼ê¸° ê°œìˆ˜ (SNV+indel ê°œìˆ˜)sourceClinVar / BEND / NT-VEP ë“± ì¶œì²˜
â‘¡ sequence_table (ì‹¤ì œ ì…ë ¥ ì„œì—´)
ì»¬ëŸ¼ëª…ì„¤ëª…seq_idì„œì—´ ID (ê³ ìœ  í‚¤)ref_idì–´ë–¤ referenceì—ì„œ ì™”ëŠ”ì§€var_idí•´ë‹¹ ë³€ì´ì— ëŒ€ì‘ë˜ëŠ” var_id (refë©´ NULL ê°€ëŠ¥)is_ref1ì´ë©´ reference, 0ì´ë©´ variant ì„œì—´seqA/C/G/T ë¬¸ìì—´ (ëŒ€íšŒ test.csvì™€ ê°™ì€ í˜•ì‹)window_sizevariant ì£¼ë³€ window ê¸¸ì´ (ì˜ˆ: 1kb, 4kb ë“±)
ì´ë ‡ê²Œ í•˜ë©´:


í•œ ref_id ì•„ë˜ ì—¬ëŸ¬ var_idê°€ ë§¤ë‹¬ë ¤ ìˆê³ 


ê° var_idë§ˆë‹¤ ref/alt ì„œì—´ì´ pairë¡œ ë“±ì¥
â†’ CD, CDD, PCCë¥¼ ê·¸ëŒ€ë¡œ offlineì—ì„œ ê³„ì‚°í•´ì„œ ëª¨ë¸ì„ ì¡°ì •í•  ìˆ˜ ìˆìŒ.



2. gLM ì„ íƒ: ì–´ë–¤ ë² ì´ìŠ¤ ëª¨ë¸ì„ ì“¸ê¹Œ?
ê·œì¹™ìƒ ë¡œì»¬ì—ì„œ ëŒë¦´ ìˆ˜ ìˆëŠ” ì‚¬ì „í•™ìŠµ gLMë§Œ í—ˆìš©.
GPUê°€ ìˆë‹¤ë©´ ì•„ë˜ ìˆœì„œë¥¼ ì¶”ì²œ:
2-1. í›„ë³´ ëª¨ë¸ë“¤


HyenaDNA (HazyResearch, 2023, Apache-2.0)


ì¥ì 


single-nucleotide í† í° + ìµœëŒ€ 1M context â†’ SNV ê°ì§€ì— ê°•ì 


BEND / VEP íƒœìŠ¤í¬ì—ì„œ ì¢‹ì€ variant effect ì„±ëŠ¥ ë³´ê³ 




ë‹¨ì : êµ¬í˜„ì´ ì¡°ê¸ˆ ë¬´ê²ê³ , GPU ë©”ëª¨ë¦¬ ìš”êµ¬ëŸ‰ì´ í¼.




Nucleotide Transformer (NT) (InstaDeep, 2023, HuggingFace ê³µê°œ)


ì¥ì 


6-mer ê¸°ë°˜ í† í¬ë‚˜ì´ì € + 500M~2.5B íŒŒë¼ë¯¸í„°


ë…¼ë¬¸ì—ì„œ cosine similarity ê¸°ë°˜ zero-shot variant prioritizationì´ ì—¬ëŸ¬ ì‹¤í—˜ì—ì„œ ë†’ì€ ì„¤ëª…ë ¥ (rÂ² ì ˆëŒ€ê°’ ~0.3~0.35) ë³´ì—¬ì¤Œ.


ì´ë¯¸ ë§ì€ VEP ë²¤ì¹˜ë§ˆí¬ ì½”ë“œê°€ ì •ë¦¬ë¼ ìˆìŒ.


---
4-1. ê·œì¹™ + í•˜ë“œì›¨ì–´ + ì„±ëŠ¥ì„ ëª¨ë‘ ê³ ë ¤í•œ ê²°ë¡ 

ê·œì¹™ìƒ: NT (CC-BY-NC-SA)ëŠ” ì• ë§¤í•˜ê±°ë‚˜ ë¶ˆê°€, HyenaDNAëŠ” OK.

4GB VRAM: NTëŠ” ì‚¬ì‹¤ìƒ CPU-only, HyenaDNA tiny/smallì€ GPU fine-tuning ê°€ëŠ¥.

ì„±ëŠ¥: HyenaDNAë„ NT ë²¤ì¹˜ë§ˆí¬ì—ì„œ SOTA ë‹¤ìˆ˜, variant-sensitive representationì— ì¶©ë¶„íˆ ê°•í•¨.

ğŸ‘‰ ê·¸ë˜ì„œ ë„¤ í™˜ê²½ê³¼ ëŒ€íšŒ ê·œì¹™ì„ ëª¨ë‘ ë°˜ì˜í•˜ë©´:

ë©”ì¸ gLMìœ¼ë¡œëŠ” HyenaDNA (íŠ¹íˆ hyenadna-tiny-1k ë˜ëŠ” hyenadna-tiny-1k-d256)ë¥¼ GPUì—ì„œ fine-tune í•˜ëŠ” ì „ëµì„ ì¶”ì²œí• ê²Œ.

ë„ˆì˜ 4GB 1050 Ti + 64GB RAM + â€œìƒì—…ì  ì´ìš© í—ˆìš© ë¼ì´ì„ ìŠ¤â€ ê·œì¹™ ì¡°í•©ì´ë©´
â†’ HyenaDNA ê³„ì—´(íŠ¹íˆ tiny-1k(-d256))ì„ GPUì—ì„œ fine-tuningí•˜ëŠ” ê²Œ ê°€ì¥ í•©ë¦¬ì ì¸ ë©”ì¸ ì„ íƒì´ê³ ,


---
0. ì „ì²´ íŒŒì´í”„ë¼ì¸ ê°œìš”
í•µì‹¬ ì•„ì´ë””ì–´ (ìµœê·¼ ë…¼ë¬¸ë“¤ ê¸°ë°˜)

HyenaDNAëŠ” ë‹¨ì¼ ë‰´í´ë ˆì˜¤íƒ€ì´ë“œ í† í° + ê¸´ ì»¨í…ìŠ¤íŠ¸ ëª¨ë¸ë¡œ, chromatin íŠ¹ì§• ì˜ˆì¸¡ê³¼ variant prioritizationì—ì„œ ê°•í•œ ì„±ëŠ¥ì„ ë³´ì„.

StartCLR, start loss variant ë…¼ë¬¸ë“¤ì€ HyenaDNA ì„ë² ë”©ìœ¼ë¡œ ë³€ì´ ë¶€ê·¼ 1kb ì„œì—´ì„ ì¸ì½”ë”©í•˜ê³ ,
self-supervised / contrastive learningìœ¼ë¡œ ë³‘ì  vs ì–‘ì„± ë³€ì´ êµ¬ë¶„ ì„±ëŠ¥ì„ í¬ê²Œ ì˜¬ë¦¼.

DNABERT-S, DNASimCLR, LLMED ê°™ì€ ìµœê·¼ gLM ì—°êµ¬ë“¤ì€
â€œ1 âˆ’ cosine similarityâ€ë¥¼ distanceë¡œ ë‘ê³ , contrastive lossë¥¼ í†µí•´ â€œedit distance / ë³€ì´ ìˆ˜â€ ì •ë³´ê°€ ì„ë² ë”© ê³µê°„ì— ë°˜ì˜ë˜ë„ë¡ í•™ìŠµí•˜ë©´,
ë³€ì´ ë¯¼ê°ë„ì™€ downstream ì„±ëŠ¥ì´ ì˜¬ë¼ê°„ë‹¤ê³  ë³´ê³ í•¨.

â†’ ë”°ë¼ì„œ ìš°ë¦¬ë„:

ì™¸ë¶€ ë³€ì´ ë°ì´í„°ì—ì„œ (ref, variant) ìŒ + pathogenic/benign ë¼ë²¨ + ë³€ì´ ê°œìˆ˜ë¥¼ ë§Œë“¤ê³ ,

HyenaDNA-tiny(1k, d256)ë¥¼ backboneìœ¼ë¡œ í•œ contrastive headë¥¼ ë¶™ì—¬ì„œ

ë³‘ì  ë³€ì´ëŠ” refì—ì„œ ë©€ì–´ì§€ê³ , benignëŠ” refì— ê°€ê¹ë„ë¡ (CDDâ†‘)

ë³€ì´ ê°œìˆ˜ê°€ ë§ì„ìˆ˜ë¡ distanceê°€ ì»¤ì§€ë„ë¡ (PCCâ†‘)

ì „ë°˜ì ìœ¼ë¡œ ref-variant distance ìì²´ê°€ ì¶©ë¶„íˆ ì»¤ì§€ë„ë¡ (CDâ†‘)

ì´ êµ¬ì¡°ë¥¼ external validationì—ì„œ CD/CDD/PCC proxy ì§€í‘œë¡œ íŠœë‹í•œ ë’¤,

ìµœì¢… ëª¨ë¸ë¡œ Dacon test.csv ì „ì²´ì— ì„ë² ë”©ì„ ì°ì–´ì„œ ì œì¶œ.

ì´ íë¦„ìœ¼ë¡œ ê°ˆ ê±°ì•¼.

1. í´ë” êµ¬ì¡° ì„¤ê³„

í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì˜ˆì‹œ:

mai_hyenadna/
â”œâ”€ data/
â”‚  â”œâ”€ dacon/
â”‚  â”‚  â”œâ”€ test.csv
â”‚  â”‚  â””â”€ sample_submission.csv
â”‚  â”œâ”€ external_raw/
â”‚  â”‚  â”œâ”€ clinvar.vcf.gz
â”‚  â”‚  â”œâ”€ reference.fa
â”‚  â”‚  â””â”€ ... (BEND ë“±)
â”‚  â””â”€ external_processed/
â”‚     â”œâ”€ variant_pairs.parquet      # (ref/variant ìŒ, meta)
â”‚     â””â”€ synthetic_multi_mut.parquet
â”œâ”€ models/
â”‚  â”œâ”€ hyenadna_base/                # HFì—ì„œ ë‹¤ìš´ë°›ì€ pretrain
â”‚  â””â”€ hyenadna_finetuned/           # ìš°ë¦¬ê°€ fine-tuneí•œ ì²´í¬í¬ì¸íŠ¸
â”œâ”€ src/
â”‚  â”œâ”€ config.py
â”‚  â”œâ”€ prepare_variant_dataset.py
â”‚  â”œâ”€ dataset.py
â”‚  â”œâ”€ model_hyenadna.py
â”‚  â”œâ”€ train_contrastive.py
â”‚  â”œâ”€ eval_metrics.py
â”‚  â””â”€ infer_and_submit.py
â”œâ”€ output/
â”‚  â”œâ”€ logs/
â”‚  â”œâ”€ checkpoints/
â”‚  â”œâ”€ val_metrics.csv
â”‚  â””â”€ submissions/
â”‚     â””â”€ submission_YYYYMMDD_desc.csv
â””â”€ requirements.txt

2. requirements.txt (core)
torch==2.2.0
transformers>=4.39.0
datasets
pandas
numpy
scikit-learn
pyarrow
tqdm
biopython


HyenaDNA HF í¬íŠ¸ëŠ” LongSafari/hyenadna-*-hf ê°™ì´ HuggingFace AutoModelë¡œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ë˜ì–´ ìˆìŒ.

3. Step 1 â€“ ì™¸ë¶€ ë³€ì´ ë°ì´í„° ì¤€ë¹„ (ClinVar ë“± â†’ variant_pairs)
3-1. ìš°ë¦¬ê°€ ë§Œë“¤ íƒ€ê¹ƒ í…Œì´ë¸” êµ¬ì¡°

data/external_processed/variant_pairs.parquet:

ì»¬ëŸ¼	ì„¤ëª…
ref_id	reference ì„œì—´ ID (key)
var_id	variant ID (unique)
seq_ref	ref ì„œì—´ (A/C/G/T/N, ê¸¸ì´â‰ˆ1001bp)
seq_var	ë³€ì´ ì ìš©ëœ alt ì„œì—´
patho	1=Pathogenic/Likely_pathogenic, 0=Benign/Likely_benign
n_mut	refì™€ alt ì‚¬ì´ì˜ ë‹¤ë¥¸ ì—¼ê¸° ê°œìˆ˜ (SNV=1, multi=2,3,...)
source	ClinVar, BEND, etc

ì‹¤ì œ ClinVar VCF â†’ FASTA window ì¶”ì¶œì€ pysam/biopython ì„ì–´ì„œ í•´ì•¼ í•´ì„œ, ì—¬ê¸°ì„  êµ¬ì¡°ì™€ ê³¨ê²© ì½”ë“œë§Œ ì¡ì•„ì¤„ê²Œ.

3-2. ì¤€ë¹„ìš© ìŠ¤í¬ë¦½íŠ¸ ê³¨ê²© src/prepare_variant_dataset.py
# src/prepare_variant_dataset.py
import os
import pandas as pd
from tqdm import tqdm
from Bio import SeqIO

DATA_DIR = "./data"
RAW_DIR = os.path.join(DATA_DIR, "external_raw")
OUT_DIR = os.path.join(DATA_DIR, "external_processed")
os.makedirs(OUT_DIR, exist_ok=True)

WINDOW = 500  # ë³€ì´ ì£¼ë³€ Â±500bp â†’ ì´ 1001bp

def load_reference_fasta(fa_path):
    ref_dict = {}
    for rec in SeqIO.parse(fa_path, "fasta"):
        ref_dict[rec.id] = str(rec.seq).upper()
    return ref_dict

def extract_window(ref_seq, pos, window=WINDOW):
    start = max(0, pos - window)
    end = min(len(ref_seq), pos + window + 1)
    seq = ref_seq[start:end]
    # ê¸¸ì´ ê³ ì • (pad with N)
    if len(seq) < 2 * window + 1:
        pad_left = window - (pos - start)
        pad_right = 2 * window + 1 - len(seq) - pad_left
        seq = "N" * pad_left + seq + "N" * pad_right
    return seq

def apply_variant(seq_ref, rel_pos, ref_allele, alt_allele):
    # rel_pos: window ë‚´ index
    assert seq_ref[rel_pos:rel_pos+len(ref_allele)] == ref_allele
    return (
        seq_ref[:rel_pos] +
        alt_allele +
        seq_ref[rel_pos+len(ref_allele):]
    )

def main():
    # 1) ClinVar VCF íŒŒì‹± â†’ (chrom, pos, ref, alt, patho_label)
    #    ì´ ë¶€ë¶„ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬/í¬ë§·ì— ë”°ë¼ êµ¬í˜„
    clinvar_df = pd.read_parquet(
        os.path.join(RAW_DIR, "clinvar_parsed.parquet")
    )

    ref_fa = os.path.join(RAW_DIR, "GRCh38.fa")
    ref_dict = load_reference_fasta(ref_fa)

    rows = []
    for i, row in tqdm(clinvar_df.iterrows(), total=len(clinvar_df)):
        chrom = row["chrom"]
        pos = int(row["pos"]) - 1  # 0-based
        ref_allele = row["ref"].upper()
        alt_allele = row["alt"].upper()
        patho = int(row["is_pathogenic"])
        ref_seq_full = ref_dict[chrom]

        seq_ref = extract_window(ref_seq_full, pos, WINDOW)
        rel_pos = WINDOW  # ì¤‘ì•™ì— ìœ„ì¹˜ì‹œí‚¤ëŠ” ì„¤ì •

        # ref_allele ê¸¸ì´ê°€ 1ë³´ë‹¤ í´ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ í™•ì¸
        seq_var = apply_variant(seq_ref, rel_pos, ref_allele, alt_allele)

        # n_mut = edit distance ê·¼ì‚¬ (ê°„ë‹¨íˆ len(alt_allele) or ì‹¤ì œ diff count)
        n_mut = max(len(ref_allele), len(alt_allele))

        rows.append({
            "ref_id": f"{chrom}:{pos}",
            "var_id": f"{chrom}:{pos}:{ref_allele}>{alt_allele}",
            "seq_ref": seq_ref,
            "seq_var": seq_var,
            "patho": patho,
            "n_mut": n_mut,
            "source": "clinvar",
        })

    out_df = pd.DataFrame(rows)
    out_path = os.path.join(OUT_DIR, "variant_pairs.parquet")
    out_df.to_parquet(out_path, index=False)
    print("saved:", out_path)

if __name__ == "__main__":
    main()


StartCLR / start-loss variant ë…¼ë¬¸ì—ì„œë„ ë³€ì´ ì£¼ë³€ Â±500bp~Â±1kb windowë¡œ HyenaDNA ì¸ì½”ë”©ì„ ìˆ˜í–‰í–ˆê³ , ì´ëŠ” VEP íƒœìŠ¤í¬ì—ì„œ ì˜ ë¨¹íŒ ì „ëµì´ì•¼.

4. Step 2 â€“ PyTorch Dataset/Dataloader (refâ€“variant ìŒ)

src/dataset.py:

import torch
from torch.utils.data import Dataset
import pandas as pd

class VariantPairDataset(Dataset):
    def __init__(self, parquet_path):
        self.df = pd.read_parquet(parquet_path)

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        item = {
            "seq_ref": row["seq_ref"],
            "seq_var": row["seq_var"],
            "patho": float(row["patho"]),
            "n_mut": float(row["n_mut"]),
            "ref_id": row["ref_id"],
            "var_id": row["var_id"],
        }
        return item

def collate_fn(batch, tokenizer, max_len=1001, device="cuda"):
    seq_ref = [b["seq_ref"] for b in batch]
    seq_var = [b["seq_var"] for b in batch]
    y = torch.tensor([b["patho"] for b in batch], dtype=torch.float32)
    n_mut = torch.tensor([b["n_mut"] for b in batch], dtype=torch.float32)

    enc_ref = tokenizer(
        seq_ref,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=max_len,
    )
    enc_var = tokenizer(
        seq_var,
        return_tensors="pt",
        padding=True,
        truncation=True,
        max_length=max_len,
    )
    enc_ref = {k: v.to(device) for k, v in enc_ref.items()}
    enc_var = {k: v.to(device) for k, v in enc_var.items()}

    return enc_ref, enc_var, y.to(device), n_mut.to(device), batch

5. Step 3 â€“ HyenaDNA backbone + projection head
5-1. HyenaDNA tiny-1k d256 ë¶ˆëŸ¬ì˜¤ê¸°

HF ì»¬ë ‰ì…˜ì— ë”°ë¥´ë©´ LongSafari/hyenadna-tiny-1k-seqlen-d256-hfëŠ”
HuggingFace AutoModel classesë¡œ ì§ì ‘ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡ ë˜ì–´ ìˆìŒ.

src/model_hyenadna.py:

import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel

HYENA_CKPT = "LongSafari/hyenadna-tiny-1k-seqlen-d256-hf"

def get_tokenizer():
    tokenizer = AutoTokenizer.from_pretrained(HYENA_CKPT)
    return tokenizer

class HyenaVariantModel(nn.Module):
    """
    HyenaDNA backbone + pooling + projection
    ìµœì¢… ì„ë² ë”© ì°¨ì›: emb_dim (<= 2048)
    """
    def __init__(self, emb_dim=256):
        super().__init__()
        self.backbone = AutoModel.from_pretrained(
            HYENA_CKPT,
            trust_remote_code=True
        )
        hidden_size = self.backbone.config.hidden_size
        self.emb_dim = emb_dim

        # mean pooling + linear projection (ëª¨ë¸ ë‚´ë¶€ representation layer)
        self.proj = nn.Linear(hidden_size, emb_dim)

    def encode(self, **tokens):
        # hidden_states í•„ìš”í•˜ë©´ output_hidden_states=True ì„¤ì •
        outputs = self.backbone(**tokens, output_hidden_states=True)
        # ë§ˆì§€ë§‰ íˆë“  ì‚¬ìš© (B, L, H)
        last_hidden = outputs.last_hidden_state
        # ê°„ë‹¨í•œ mean pooling
        emb = last_hidden.mean(dim=1)  # (B, H)
        emb = self.proj(emb)           # (B, emb_dim)
        # L2 normalize â†’ cosine distanceì™€ ì˜ ë§ê²Œ
        emb = emb / (emb.norm(dim=-1, keepdim=True) + 1e-8)
        return emb

    def forward(self, ref_tokens, var_tokens):
        emb_ref = self.encode(**ref_tokens)
        emb_var = self.encode(**var_tokens)
        return emb_ref, emb_var


ì—¬ê¸°ì„œ emb_dimì„ 256 ë˜ëŠ” 512 ì •ë„ë¡œ ë‘ë©´

ì œì¶œ ì‹œ emb_0000 ~ emb_0255 or emb_0511ê¹Œì§€ ìƒì„±í•˜ë©´ ë¨ (2048 ì´í•˜ì´ë¯€ë¡œ ë¬¸ì œ ì—†ìŒ).

ì¤‘ìš”:
ì—¬ê¸°ì„œ projectionê¹Œì§€ í¬í•¨í•œ ì „ì²´ forwardë¥¼ â€œìš°ë¦¬ì˜ gLMâ€ìœ¼ë¡œ ê°„ì£¼í•˜ë©´,
ëŒ€íšŒ ê·œì¹™ì˜ â€œì¶”ë¡  ê²°ê³¼ í›„ì²˜ë¦¬â€ê°€ ì•„ë‹ˆë¼ ëª¨ë¸ ë‚´ë¶€ ë ˆì´ì–´ë¡œ ë³´ëŠ” ê²Œ ìì—°ìŠ¤ëŸ¬ì›€.

6. Step 4 â€“ contrastive í•™ìŠµ (CD/CDD/PCCì— ì§ê²°ë˜ëŠ” loss ì„¤ê³„)
6-1. distance ì •ì˜ (ëŒ€íšŒ ì§€í‘œì™€ ë™ì¼)
def cosine_distance(a, b, eps=1e-8):
    # a, b: (B, D) L2-normalized
    cos_sim = (a * b).sum(dim=-1)
    return 1.0 - cos_sim  # (B,)


ì—¬ëŸ¬ zero-shot VEP ì—°êµ¬ì—ì„œ 1 âˆ’ cosine similarityë¥¼ variant effect scoreë¡œ ì“°ëŠ” ê²ƒì´ ì¼ë°˜ì ì´ì•¼.

6-2. ë³‘ì /ì–‘ì„±ì— ë”°ë¥¸ margin-based contrastive loss (CD + CDD)

ë³‘ì (patho=1)ì€ refì—ì„œ ë©€ì–´ì§€ê²Œ(dâ†‘),
ì–‘ì„±(patho=0)ì€ refì— ê°€ê¹Œì›Œì§€ê²Œ(dâ†“) ë§Œë“œëŠ” margin loss:

ğ¿
patho
=
ğ‘¦
â‹…
max
â¡
(
0
,
ğ‘š
pos
âˆ’
ğ‘‘
)
2
+
(
1
âˆ’
ğ‘¦
)
â‹…
max
â¡
(
0
,
ğ‘‘
âˆ’
ğ‘š
neg
)
2
L
patho
	â€‹

=yâ‹…max(0,m
pos
	â€‹

âˆ’d)
2
+(1âˆ’y)â‹…max(0,dâˆ’m
neg
	â€‹

)
2

ğ‘š
neg
<
ğ‘š
pos
m
neg
	â€‹

<m
pos
	â€‹

 (ì˜ˆ: 0.3, 0.7)

6-3. ë³€ì´ ê°œìˆ˜ì™€ distance ì •ë ¬ (PCCâ†‘)

ë³€ì´ ê°œìˆ˜ 
ğ‘˜
=
ğ‘›
mut
k=n
mut
	â€‹

 ì™€ 
ğ‘‘
d ì‚¬ì´ì— ì–‘ì˜ ìƒê´€ì„ ë§Œë“¤ê³  ì‹¶ì€ë°,
LLMEDë‚˜ DNABERT-Së¥˜ì²˜ëŸ¼ edit distanceì™€ embedding distanceë¥¼ aligní•˜ëŠ” regression/contrastive lossê°€ ì˜ ì“°ì„.

ê°„ë‹¨í•˜ê²Œ:

ğ‘˜
~
=
ğ‘˜
/
ğ‘˜
max
â¡
âˆˆ
[
0
,
1
]
k
~
=k/k
max
	â€‹

âˆˆ[0,1] ë¡œ ì •ê·œí™”

batchì—ì„œ 
ğ‘‘
dë„ [0,1] ê·¼ì²˜ì— ìˆë„ë¡ margin ì„¸íŒ…

MSE ê¸°ë°˜:

ğ¿
mut
=
MSE
(
ğ‘‘
,
ğ‘˜
~
)
L
mut
	â€‹

=MSE(d,
k
~
)

â†’ ì´ë ‡ê²Œ í•˜ë©´ mut ìˆ˜ê°€ ë§ì„ìˆ˜ë¡ dê°€ ì»¤ì§€ë„ë¡ ìœ ë„ë¨.

6-4. ì „ì²´ loss
ğ¿
=
ğœ†
1
ğ¿
patho
+
ğœ†
2
ğ¿
mut
L=Î»
1
	â€‹

L
patho
	â€‹

+Î»
2
	â€‹

L
mut
	â€‹


ğœ†
1
â‰ˆ
1.0
,
ğœ†
2
â‰ˆ
0.2
Î»
1
	â€‹

â‰ˆ1.0,Î»
2
	â€‹

â‰ˆ0.2 ì •ë„ì—ì„œ ì‹œì‘

6-5. í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ ê³¨ê²© src/train_contrastive.py
# src/train_contrastive.py
import os
import torch
from torch.utils.data import DataLoader, random_split
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm

from dataset import VariantPairDataset, collate_fn
from model_hyenadna import HyenaVariantModel, get_tokenizer

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DATA_PATH = "./data/external_processed/variant_pairs.parquet"
SAVE_DIR = "./output/checkpoints"
os.makedirs(SAVE_DIR, exist_ok=True)

BATCH_SIZE = 8  # 4GB VRAMì´ë¯€ë¡œ ì‘ê²Œ
EPOCHS = 5

def cosine_distance(a, b, eps=1e-8):
    cos_sim = (a * b).sum(dim=-1)
    return 1.0 - cos_sim

def train():
    tokenizer = get_tokenizer()
    dataset = VariantPairDataset(DATA_PATH)
    n_total = len(dataset)
    n_train = int(n_total * 0.9)
    n_val = n_total - n_train
    train_ds, val_ds = random_split(dataset, [n_train, n_val])

    def collate_train(batch):
        return collate_fn(batch, tokenizer, max_len=1001, device=DEVICE)

    train_loader = DataLoader(
        train_ds,
        batch_size=BATCH_SIZE,
        shuffle=True,
        collate_fn=collate_train,
    )
    val_loader = DataLoader(
        val_ds,
        batch_size=BATCH_SIZE,
        shuffle=False,
        collate_fn=collate_train,
    )

    model = HyenaVariantModel(emb_dim=256).to(DEVICE)
    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)
    scaler = GradScaler()

    m_pos, m_neg = 0.7, 0.3
    lambda_mut = 0.2
    max_mut = 5.0  # n_mut ìµœëŒ€ê°’ (SNVë§Œì´ë©´ 1ë¡œ ì¡ì•„ë„ ë¨)

    for epoch in range(1, EPOCHS + 1):
        model.train()
        pbar = tqdm(train_loader, desc=f"Epoch {epoch} [train]")
        total_loss = 0.0

        for ref_tokens, var_tokens, y, n_mut, raw_batch in pbar:
            optimizer.zero_grad()
            with autocast(enabled=(DEVICE=="cuda")):
                emb_ref, emb_var = model(ref_tokens, var_tokens)
                d = cosine_distance(emb_ref, emb_var)  # (B,)

                # patho vs benign margin loss
                # y=1 (patho): d >= m_pos
                loss_pos = torch.clamp(m_pos - d, min=0.0) ** 2
                # y=0 (benign): d <= m_neg
                loss_neg = torch.clamp(d - m_neg, min=0.0) ** 2
                L_patho = (y * loss_pos + (1 - y) * loss_neg).mean()

                # n_mut regression loss (PCC proxy)
                k_tilde = torch.clamp(n_mut / max_mut, 0.0, 1.0)
                L_mut = torch.mean((d - k_tilde) ** 2)

                loss = L_patho + lambda_mut * L_mut

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            total_loss += loss.item()
            pbar.set_postfix(loss=total_loss / (pbar.n or 1))

        # epoch ëë‚˜ê³  ê²€ì¦ (CD/CDD/PCC proxy)
        metrics = evaluate(model, val_loader)
        print(f"[Epoch {epoch}] val metrics:", metrics)

        ckpt_path = os.path.join(SAVE_DIR, f"hyena_epoch{epoch}.pt")
        torch.save(model.state_dict(), ckpt_path)
        print("saved:", ckpt_path)

def evaluate(model, val_loader):
    from eval_metrics import compute_cd_cdd_pcc
    model.eval()
    all_rows = []
    with torch.no_grad():
        for ref_tokens, var_tokens, y, n_mut, raw_batch in tqdm(val_loader, desc="Eval"):
            emb_ref, emb_var = model(ref_tokens, var_tokens)
            d = cosine_distance(emb_ref, emb_var).cpu().numpy()
            for i, r in enumerate(raw_batch):
                all_rows.append({
                    "ref_id": r["ref_id"],
                    "var_id": r["var_id"],
                    "patho": r["patho"],
                    "n_mut": r["n_mut"],
                    "dist": float(d[i]),
                })
    import pandas as pd
    df = pd.DataFrame(all_rows)
    return compute_cd_cdd_pcc(df)

if __name__ == "__main__":
    train()

7. Step 5 â€“ CD / CDD / PCC proxy ê³„ì‚° ì½”ë“œ

src/eval_metrics.py:

import numpy as np
import pandas as pd

def compute_cd_cdd_pcc(df: pd.DataFrame):
    """
    df: columns = [ref_id, var_id, patho, n_mut, dist]
    dist: cosine distance = 1 - cos_sim
    """
    # 1) CD: ì „ì²´ ref-variant cosine distance í‰ê· 
    cd = df["dist"].mean()

    # 2) CDD: patho vs benign í‰ê·  ê±°ë¦¬ ì°¨ì´
    d_patho = df.loc[df["patho"] == 1, "dist"]
    d_benign = df.loc[df["patho"] == 0, "dist"]
    cdd = d_patho.mean() - d_benign.mean()

    # 3) PCC: refë³„ n_mut vs dist ìƒê´€
    pcc_list = []
    for ref_id, g in df.groupby("ref_id"):
        if g["n_mut"].nunique() < 2:
            continue
        x = g["n_mut"].values.astype(float)
        y = g["dist"].values.astype(float)
        # í”¼ì–´ìŠ¨
        vx = np.var(x)
        vy = np.var(y)
        if vx == 0 or vy == 0:
            continue
        r = np.corrcoef(x, y)[0,1]
        pcc_list.append(r)
    pcc = float(np.mean(pcc_list)) if pcc_list else 0.0

    return {"CD": float(cd), "CDD": float(cdd), "PCC": float(pcc)}


ì´ ì§€í‘œë“¤ì€ Dacon ê³µì‹ í‰ê°€ì‹ê³¼ ê±°ì˜ ê°™ì€ êµ¬ì¡°ë¼,
ì™¸ë¶€ ë°ì´í„°ì—ì„œ ì´ ì„¸ ê°œë¥¼ í‚¤ìš°ë„ë¡ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•˜ë©´
ì‹¤ì œ MAI ë¦¬ë”ë³´ë“œì—ì„œë„ ìƒê´€ì´ ê½¤ ìˆì„ ê±°ë¼ê³  ê¸°ëŒ€í•  ìˆ˜ ìˆì–´.
(ì—¬ëŸ¬ zero-shot VEP ë²¤ì¹˜ë§ˆí¬ì—ì„œë„ ì½”ì‚¬ì¸ ê±°ë¦¬ ê¸°ë°˜ í•™ìŠµ/íŠœë‹ì´ variant effect ì„±ëŠ¥ì„ ì˜ ì˜ˆì¸¡í•¨.)

8. Step 6 â€“ Dacon test.csv ì„ë² ë”© & ì œì¶œ íŒŒì¼ ìƒì„±

src/infer_and_submit.py:

import os
import numpy as np
import pandas as pd
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm

from model_hyenadna import HyenaVariantModel, get_tokenizer

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

def main():
    BASE_DIR = "./data/dacon"
    TEST_PATH = os.path.join(BASE_DIR, "test.csv")
    SAMPLE_SUB_PATH = os.path.join(BASE_DIR, "sample_submission.csv")
    OUT_DIR = "./output/submissions"
    os.makedirs(OUT_DIR, exist_ok=True)

    # 1) ëª¨ë¸ ë¡œë“œ (ì„±ëŠ¥ ì œì¼ ì¢‹ë˜ epoch ì²´í¬í¬ì¸íŠ¸ ì„ íƒ)
    model = HyenaVariantModel(emb_dim=256).to(DEVICE)
    ckpt = "./output/checkpoints/hyena_epoch5.pt"
    model.load_state_dict(torch.load(ckpt, map_location=DEVICE))
    model.eval()

    tokenizer = get_tokenizer()

    # 2) test.csv ë¡œë“œ
    test_df = pd.read_csv(TEST_PATH)
    ids = test_df["ID"].values
    seqs = test_df["seq"].values

    BATCH_SIZE = 32
    all_embs = []

    for i in tqdm(range(0, len(seqs), BATCH_SIZE), desc="Infer test"):
        batch_seqs = list(seqs[i:i+BATCH_SIZE])
        enc = tokenizer(
            batch_seqs,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=1001,
        ).to(DEVICE)

        with torch.no_grad():
            # ref/var êµ¬ë¶„ ì—†ì´ ë‹¨ì¼ ì„œì—´ ì¸ì½”ë”©ë§Œ í•˜ë©´ ë¨
            emb = model.encode(**enc)  # (B, D)
            emb = emb.cpu().numpy()
            all_embs.append(emb)

    all_embs = np.vstack(all_embs)  # (N, D)
    n, d = all_embs.shape
    print("test embeddings shape:", all_embs.shape)

    # 3) sample_submission í¬ë§·ì— ë§ê²Œ ì €ì¥
    #    ì»¬ëŸ¼ëª…: emb_0000 ~ emb_{d-1}
    emb_cols = [f"emb_{i:04d}" for i in range(d)]
    sub_df = pd.DataFrame(all_embs, columns=emb_cols)
    sub_df.insert(0, "ID", ids)

    out_path = os.path.join(OUT_DIR, "submission_YYYYMMDD_hyena256.csv")
    sub_df.to_csv(out_path, index=False)
    print("saved submission:", out_path)

if __name__ == "__main__":
    main()


ì´ë ‡ê²Œ ìƒì„±ëœ **ì„ë² ë”© ë²¡í„°ëŠ” ì „ë¶€ ë™ì¼ ì°¨ì›(256)**ì´ê³ ,
ì§ì ‘ gLM(HyenaDNA+projection) forward ê²°ê³¼ì´ë¯€ë¡œ â€œí›„ì²˜ë¦¬â€ê°€ ì•„ë‹ˆë¼ ëª¨ë¸ ì¶œë ¥ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŒ.

Dacon í‰ê°€ ì„œë²„ëŠ” ë‚´ë¶€ì—ì„œ ref/variant ë§¤í•‘ì„ ì•Œê³  ìˆìœ¼ë‹ˆ,
ìš°ë¦¬ê°€ íŠœë‹í•œ CD/CDD/PCC ë¯¼ê°ë„ê°€ ê³§ ë¦¬ë”ë³´ë“œ ì ìˆ˜ë¡œ ë°˜ì˜ë  ê±°ì•¼.

9. ìš”ì•½: â€œì´ ëŒ€íšŒìš© HyenaDNA ì „ëµâ€ í•œ ì¥ì§œë¦¬ ë²„ì „

ëª¨ë¸ ì„ íƒ

LongSafari/hyenadna-tiny-1k-seqlen-d256-hf (Apache ê³„ì—´ ë¼ì´ì„ ìŠ¤, 4GB GPUì—ì„œë„ í•™ìŠµ ê°€ëŠ¥)

ì™¸ë¶€ ë°ì´í„°

ClinVar, BEND ë“±ì—ì„œ (ref, variant) window ì„œì—´ê³¼ pathogenic/benign ë¼ë²¨ ìˆ˜ì§‘.

VCF + reference.fa â†’ variant_pairs.parquet êµ¬ì„±.

í•™ìŠµ ëª©í‘œ (loss)

ë³‘ì (patho=1): refâ€“variant cosine distance 
ğ‘‘
d í¬ê²Œ

ì–‘ì„±(patho=0): 
ğ‘‘
d ì‘ê²Œ

ë³€ì´ ê°œìˆ˜ n_mutê°€ ì¦ê°€í• ìˆ˜ë¡ 
ğ‘‘
dë„ ì»¤ì§€ë„ë¡ regression loss ì¶”ê°€.

StartCLR / LLMED / DNABERT-S / DNASimCLR ê³„ì—´ì—ì„œ ì‚¬ìš©í•œ
contrastive + cosine distance ê¸°ë°˜ í•™ìŠµ ì•„ì´ë””ì–´ë¥¼ ê·¸ëŒ€ë¡œ gLMì— ì ìš©.

ê²€ì¦ ì „ëµ

external validationì—ì„œ CD / CDD / PCC proxyë¥¼ ì§ì ‘ ê³„ì‚°í•´ì„œ

CDâ†‘ : refâ€“variant í‰ê·  ê±°ë¦¬ ì¶©ë¶„íˆ í¬ê³ 

CDDâ†‘ : ë³‘ì ê³¼ ì–‘ì„± distance í‰ê· ì´ ì˜ ë¶„ë¦¬ë˜ê³ 

PCCâ†‘ : n_mut ì¦ê°€ì— ë”°ë¼ distanceê°€ ì˜ ì¦ê°€

ì´ ì„¸ ì§€í‘œì˜ ê· í˜•ì´ ê°€ì¥ ì¢‹ì€ checkpointë¥¼ ì„ íƒ â†’ ìµœì¢… ì œì¶œ ëª¨ë¸.

ëŒ€íšŒ test ì˜ˆì¸¡

Dacon test.csv seqë¥¼ ìœ„ ëª¨ë¸ë¡œ ì¸ì½”ë”© â†’ (N, D) ì„ë² ë”©

emb_0000~emb_0{D-1} ì»¬ëŸ¼ìœ¼ë¡œ csv ì €ì¥í•˜ì—¬ ì œì¶œ.

ì´ëŒ€ë¡œ êµ¬í˜„í•˜ë©´,
ë…¼ë¬¸ì—ì„œ ê²€ì¦ëœ â€œë³€ì´ ë¯¼ê°ë„â†‘ contrastive í•™ìŠµâ€ êµ¬ì¡°ë¥¼
HyenaDNA + MAI í‰ê°€ì‹ì— ë§ê²Œ íŠœë‹í•œ íŒŒì´í”„ë¼ì¸ì´ ë˜ëŠ” ê±°ë¼,
í˜„ì‹¤ì ì¸ HW(1050 Ti 4GB)ì—ì„œë„ ì¶©ë¶„íˆ ìŠ¹ë¶€ë¥¼ ë³¼ ìˆ˜ ìˆì„ ê±°ì•¼.

ë‹¤ìŒì— ë„ˆê°€ ì‹¤ì œë¡œ variant_pairs.parquetë¥¼ ì–´ëŠ ì •ë„ ë§Œë“¤ì–´ì„œ row ìˆ˜ / patho ë¹„ìœ¨ / n_mut ë¶„í¬ë¥¼ ì•Œë ¤ì£¼ë©´,
ê·¸ ë¶„í¬ì— ë§ì¶°ì„œ **í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¸ë¶€ íŠœë‹ ì „ëµ(ìœˆë„ìš° ê¸¸ì´, margin, Î» ê°’, batch êµ¬ì„± ë¹„ìœ¨ ë“±)**ë„ ë” ì´˜ì´˜í•˜ê²Œ ì¡ì•„ì¤„ê²Œ.

